{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In this Notebook, you will learn\n",
        "- What does __Training__ mean?\n",
        "- Introduction to trainers,  some of their differences, and how to decide which one to use.\n",
        "- How hyper-parameters impact training performance.\n",
        "- How to use AutoML to simplify your training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is __Training__ actually mean.\n",
        "Before diving into code, let's first talk a little about what does \"train a model\" actually mean. \n",
        "\n",
        "From code aspect, \"train a model\" usually means call `model.fit(X, y)` in most of python machine learning packages, where `X` is feature array and `y` is label, or `model.Fit(X)` in ML.Net, where `X` is an `IDataView` which includes both feature and label.\n",
        "\n",
        "In ML.NET, you kick off a training job by calling the `Fit` method. So what happen when you call `Fit`? To answer that question you first need to define what a good model means. Generally, a good model means the values it predicts are very close to the actual value. The value to predict is commonly known as the **target** or **label**. For classification, a good model means it can classify data points correctly. For regression, a good model means its predicted numerical value is very close to actual numerical value. So we just need to find a way to quantify how close or how different the predicted and actual values are.\n",
        "\n",
        "In machine learning, the difference or distance between predicted and actual label is usually called **loss**. Similar to training algorithms, you use different loss measures based on the task. For classification softmax is a common loss measure. For regression, Root Mean Squared Error (RMSE) is a common loss measure. In general though, they are all metrics to quantify the distance between the predicted and actual value. In most of cases, a **lower loss means a better model**. For more information, see the [ML.NET evaluation metrics guide](https://docs.microsoft.com/dotnet/machine-learning/resources/metrics).\n"
        "\n",
        "So what `Fit` does is apply an algorithm to your data to identify patterns and estimate a function that lowers the loss. When you train a model, you want to decrease its loss to make the prediction of that model closer to the actual value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainers in ML.Net\n",
        "ML.NET provides a variety of trainers. You can find most of them under the [StandardTrainersCatalog](https://docs.microsoft.com/dotnet/api/microsoft.ml.standardtrainerscatalog?view=ml-dotnet). Examples of trainers include linear trainers like `SDCA`, `Lbfgs`, `LinearSvm` and tree-based non-linear trainerd like `FastTree`, `RandomForest` and `LightGbm`. Generally, each trainer's capability is different. Non-linear models sometimes have better training performance (lower loss) than linear ones, but it doesn't always mean they are always the better choice. Picking the right trainer to build the best model for your data requires many attempts of trial and error.\n",
        "\n",
        "### Hyper-parameter optimization\n",
        "Choosing the right trainer impacts your final training performance. Choosing the right hyper-parameters also has a huge impact over the final training performance, especially for tree-base trainers. A hyper-parameter is a parameter set prior to training to help guide the training process and assist the algorithm in estimating the function that best fits your data. Hyper-parameters are important because the ability of these trainers to fit a specific dataset is largely depends on their hyper parameters. For example, larger `numberOfLeaves` in `LightGbm` produces a larger model and usually enables it to fit on a more complex dataset, but it might have countereffect on small dataset and cause **overfitting**. Conversely, if the dataset is complex but you set a small `numberOfLeaves`, it might impair `LightGbm`'s ability on fitting that dataset and cause **underfit**.\n",
        "\n",
        "The process of finding the best configuration for your trainer is known as hyper-parameter optimization (HPO). Like the process of choosing your trainer it involves a lot of trial and error. The built-in Automated ML (AutoML) capabilities in ML.NET simplify the HPO process.\n",
        "\n",
        "### Overfitting and Underfitting\n",
        "Overfitting and underfitting are the two most common problems you encounter when training a model. Underfitting means the selected trainer is not capable enough to fit training dataset and usually result in a high loss during training and low score/metric on test dataset. To resolve this you need to either select a more powerful model or perform more feature engineering. Overfitting is the opposite, which happens when model learns the training data too well. This usually results in low loss metric during training but high loss on test dataset.\n". 
        
        A good analogy for these concepts is studying for an exam. Let's say you knew the questions and answers ahead of time. After studying, you take the test and get a perfect score. Great news! However, when you're given the exam again with the questions rearranged and with slightly different wording you get a lower score. That suggests you memorized the answers and didn't actually learn the concepts you were being tested on. This is an example of overfitting. Underfitting is the opposite where the study materials you were given don't accurately represent what you're evaluated on for the exam. As a result, you resort to guessing the answers since you don't have enough knowledge to answer correctly.  
        "\n",
        "In the next section, we will go through two examples. The first example trains a regression model on a linear dataset using both linear and more advanced non-linear trainers to highlight the importance of selecting the right trainer. The second example trains a regression model on a non-linear dataset using `LightGbm` with different hyper-parameters to show the importance of hyper-parameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Linear regression\n",
        "In the below section, we are going to show the difference of trainers via a linear regression task. First, we fit the linear dataset with the linear trainer, `SDCA`. Then we git the linear dataset with `LightGbm`, a tree-base non-linear trainer. Their performance is evaluated against a test dataset. The code below:\n",
        "- Creates a linear dataset and splits it into train/test sets\n",
        "- Create training pipelines using `SDCA` and `LightGbm`\n",
        "- Trains both `SDCA` and `LightGbm` on the linear training set, and evaluates them on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><div><strong>Restore sources</strong><ul><li><span>https://mlnetcli.blob.core.windows.net/mlnetcli/index.json</span></li><li><span>https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json</span></li><li><span>https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet5/nuget/v3/index.json</span></li><li><span>https://pkgs.dev.azure.com/dnceng/public/_packaging/MachineLearning/nuget/v3/index.json</span></li></ul></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Data.Analysis, 0.20.0-preview.22226.2</span></li><li><span>Microsoft.ML.AutoML, 0.20.0-preview.22226.2</span></li><li><span>MLNetAutoML.InteractiveExtension, 0.1.1</span></li><li><span>XPlot.Plotly.Interactive, 4.0.6</span></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `MLNetAutoML.InteractiveExtension.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `XPlot.Plotly.Interactive.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Configuring PowerShell Kernel for XPlot.Plotly integration."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Installed support for XPlot.Plotly."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `Microsoft.Data.Analysis.Interactive.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// install dependencies and import using statement\n",
        "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet5/nuget/v3/index.json\" \n",
        "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json\"\n",
        "#i \"nuget:https://mlnetcli.blob.core.windows.net/mlnetcli/index.json\"\n",
        "\n",
        "// add nightly build for ml.net\n",
        "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/MachineLearning/nuget/v3/index.json\"\n",
        "\n",
        "#r \"nuget:MLNetAutoML.InteractiveExtension,0.1.1\"\n",
        "#r \"nuget:XPlot.Plotly.Interactive,4.0.6\"\n",
        "#r \"nuget:Microsoft.ML.AutoML,0.20.0-preview.22226.2\"\n",
        "#r \"nuget:Microsoft.Data.Analysis,0.20.0-preview.22226.2\"\n",
        "\n",
        "// Import usings.\n",
        "using Microsoft.Data.Analysis;\n",
        "using System;\n",
        "using System.IO;\n",
        "using Microsoft.ML;\n",
        "using Microsoft.ML.AutoML;\n",
        "using Microsoft.ML.Data;\n",
        "using MLNetAutoML.InteractiveExtension;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create linear dataset\n",
        "The code below creates a linear dataset with a random residual. The dataset is loaded into train and test DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table id=\"table_637891709650310641\"><thead><tr><th><i>index</i></th><th>X</th><th>y</th></tr></thead><tbody><tr><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">-1000</div></td><td><div class=\"dni-plaintext\">-99997.734</div></td></tr><tr><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">-999.9</div></td><td><div class=\"dni-plaintext\">-99986.83</div></td></tr><tr><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">-999.8</div></td><td><div class=\"dni-plaintext\">-99977.32</div></td></tr><tr><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">-999.7</div></td><td><div class=\"dni-plaintext\">-99969.42</div></td></tr><tr><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">-999.60004</div></td><td><div class=\"dni-plaintext\">-99962.94</div></td></tr><tr><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">-999.5</div></td><td><div class=\"dni-plaintext\">-99949.414</div></td></tr><tr><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">-999.4</div></td><td><div class=\"dni-plaintext\">-99935.94</div></td></tr><tr><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">-999.3</div></td><td><div class=\"dni-plaintext\">-99930.58</div></td></tr><tr><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">-999.2</div></td><td><div class=\"dni-plaintext\">-99915.23</div></td></tr><tr><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">-999.10004</div></td><td><div class=\"dni-plaintext\">-99912.266</div></td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var rand = new Random(0);\n",
        "var context =new MLContext(seed: 1);\n",
        "var x = Enumerable.Range(-10000, 10000).Select(_x => _x * 0.1f).ToArray();\n",
        "var y = x.Select(_x => 100 * _x + (rand.NextSingle() - 0.5f) * 10).ToArray();\n",
        "var df = new DataFrame();\n",
        "df[\"X\"] = DataFrameColumn.Create(\"X\", x);\n",
        "df[\"y\"] = DataFrameColumn.Create(\"y\", y);\n",
        "var trainTestSplit = context.Data.TrainTestSplit(df);\n",
        "df.Head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct pipeline\n",
        "The code below shows how to construct training pipelines for both `SDCA` and `LightGbm`. The `Concatenate` transformer is required to convert a `single` column into `Vector<single>` type, which is the expected feature type for both `SDCA` and `LightGbm` regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var sdcaPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.Sdca(\"y\"));\n",
        "var lgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\"));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate model\n",
        "The code below first trains `sdcaPipeline` and `lgbmPipeline` which are created above, then evaluate their performance on test dataset by calcuate `Root Mean Square Loss` between predicted and truth value. We can see that `SDCA` has better performance with a significant lower `Root Mean Square Loss` comparing with `LightGbm`, even it's a simple, linear model. This is because the training dataset is also linear, so `SDCA` can fit the dataset better than `LightGbm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdca rmse on trainset: 2.889902597526722, lgbm rmse on trainset: 117.9591141671402\r\n",
            "sdca rmse on testset: 2.924784890718881, lgbm rmse on testset: 119.860254032606\r\n"
          ]
        }
      ],
      "source": [
        "var sdcaModel = sdcaPipeline.Fit(trainTestSplit.TrainSet);\n",
        "var lgbmModel = lgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "\n",
        "// evaluate on train set\n",
        "var sdcaTrainEval = sdcaModel.Transform(trainTestSplit.TrainSet);\n",
        "var sdcaTrainMetric = context.Regression.Evaluate(sdcaTrainEval, \"y\");\n",
        "\n",
        "var lgbmTrainEval = lgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var lgbmTrainMetric = context.Regression.Evaluate(lgbmTrainEval, \"y\");\n",
        "\n",
        "Console.WriteLine($\"sdca rmse on trainset: {sdcaTrainMetric.RootMeanSquaredError}, lgbm rmse on trainset: {lgbmTrainMetric.RootMeanSquaredError}\");\n",
        "\n",
        "// evaluate on test set\n",
        "var sdcaTestEval = sdcaModel.Transform(trainTestSplit.TestSet);\n",
        "var sdcaTestMetric = context.Regression.Evaluate(sdcaTestEval, \"y\");\n",
        "\n",
        "var lgbmTestEval = lgbmModel.Transform(trainTestSplit.TestSet);\n",
        "var lgbmTestMetric = context.Regression.Evaluate(lgbmTestEval, \"y\");\n",
        "Console.WriteLine($\"sdca rmse on testset: {sdcaTestMetric.RootMeanSquaredError}, lgbm rmse on testset: {lgbmTestMetric.RootMeanSquaredError}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Non-linear regression on LightGbm.\n",
        "This example is to show the importance of hyper-parameter optimization. We will first create a non-linear dataset and two pipelines. One pipeline has `LightGbm` with `numberOfLeaves` set to 10, the other's set to 1000. Then train both pipelines with the same train dataset and Comparing their training performance by evaluating them on the same test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create non-linear dataset\n",
        "The code below artifacts non-linear dataset with a random residual, and loaded it as train/test `DataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table id=\"table_637891709701949023\"><thead><tr><th><i>index</i></th><th>X</th><th>y</th></tr></thead><tbody><tr><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">-1000</div></td><td><div class=\"dni-plaintext\">100000000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">-999.9</div></td><td><div class=\"dni-plaintext\">99980000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">-999.8</div></td><td><div class=\"dni-plaintext\">99960000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">-999.7</div></td><td><div class=\"dni-plaintext\">99940010</div></td></tr><tr><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">-999.60004</div></td><td><div class=\"dni-plaintext\">99920020</div></td></tr><tr><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">-999.5</div></td><td><div class=\"dni-plaintext\">99900024</div></td></tr><tr><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">-999.4</div></td><td><div class=\"dni-plaintext\">99880050</div></td></tr><tr><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">-999.3</div></td><td><div class=\"dni-plaintext\">99860050</div></td></tr><tr><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">-999.2</div></td><td><div class=\"dni-plaintext\">99840070</div></td></tr><tr><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">-999.10004</div></td><td><div class=\"dni-plaintext\">99820090</div></td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var rand = new Random(0);\n",
        "var context =new MLContext(seed: 1);\n",
        "var x = Enumerable.Range(-10000, 10000).Select(_x => _x * 0.1f).ToArray();\n",
        "var y = x.Select(_x => 100 * _x * _x + (rand.NextSingle() - 0.5f) * 10).ToArray();\n",
        "var df = new DataFrame();\n",
        "df[\"X\"] = DataFrameColumn.Create(\"X\", x);\n",
        "df[\"y\"] = DataFrameColumn.Create(\"y\", y);\n",
        "var trainTestSplit = context.Data.TrainTestSplit(df);\n",
        "df.Head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct pipeline\n",
        "The code below shows how to construct pipelines for `LightGbm` with different hyper parameters. The `Concatenate` transformer is necessary because it transfer a `single` column into `Vector<single>` type, which is the accepted feature type for `LightGbm` regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var smallLgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\", numberOfLeaves: 10));\n",
        "var largeLgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\", numberOfLeaves: 1000));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate model\n",
        "The code below first trains `smallLgbmPipeline` and `largeLgbmPipeline` which are created above, then evaluate their performance on test dataset by calcuate `Root Mean Square Loss` between predicted and truth value. We can see that large lgbm has better performance with a lower rmse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "small lgbm rmse on testset: 173938.52924678137, large lgbm rmse on testset: 132927.2510939994\r\n"
          ]
        }
      ],
      "source": [
        "var smallLgbmModel = smallLgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "var largeLgbmModel = largeLgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "\n",
        "// evaluate on test set\n",
        "var smallTestEval = smallLgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var smallLgbmMetric = context.Regression.Evaluate(smallTestEval, \"y\");\n",
        "\n",
        "var largeLgbmEval = largeLgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var largeLgbmMetric = context.Regression.Evaluate(largeLgbmEval, \"y\");\n",
        "\n",
        "Console.WriteLine($\"small lgbm rmse on testset: {smallLgbmMetric.RootMeanSquaredError}, large lgbm rmse on testset: {largeLgbmMetric.RootMeanSquaredError}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use AutoML to simplify hyper-parameter optimization.\n",
        "Hyper-parameter optimization is tedious while important, and it's also something that can be done automatically. Using built-in `AutoMLExperiment` can greatly simplify hpo process. `AutoMLExperiment` applies the latest research from MSR so it can conduct swift, accurate and thorough hyper-parameter optimization in a limited time budget.\n",
        "\n",
        "The code below shows how to use `AutoMLExperiment` for hpo and explore a better configuration for `LightGbm` on the non-linear dataset used in Example 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><h3>Best Run</h3><p>Trial: 37</p><p>Pipeline: Unknown=&gt;LightGbmRegression</p><h3>Active Run</h3><p>Trial: 41</p><p>Pipeline: Unknown=&gt;LightGbmRegression</p><p>Parameters: {\r\n",
              "  &quot;0&quot;: {},\r\n",
              "  &quot;1&quot;: {\r\n",
              "    &quot;NumberOfLeaves&quot;: 5387,\r\n",
              "    &quot;MinimumExampleCountPerLeaf&quot;: 20,\r\n",
              "    &quot;LearningRate&quot;: 0.9999997766729865,\r\n",
              "    &quot;NumberOfTrees&quot;: 10671,\r\n",
              "    &quot;SubsampleFraction&quot;: 0.9907605877767208,\r\n",
              "    &quot;MaximumBinCountPerFeature&quot;: 208,\r\n",
              "    &quot;FeatureFraction&quot;: 0.99999999,\r\n",
              "    &quot;L1Regularization&quot;: 2E-10,\r\n",
              "    &quot;L2Regularization&quot;: 0.1170799586462287,\r\n",
              "    &quot;LabelColumnName&quot;: &quot;y&quot;,\r\n",
              "    &quot;FeatureColumnName&quot;: &quot;Features&quot;\r\n",
              "  }\r\n",
              "}</p></div><!DOCTYPE html>\r\n",
              "<div style=\"width: 500px; height: 500px;\" id=\"1b600c5e-baef-44ed-8d2e-6f314fbc7cba\"></div><script type=\"text/javascript\">\r\n",
              "\n",
              "var renderPlotly = function() {\n",
              "    var xplotRequire = require.config({context:'xplot-3.0.1',paths:{plotly:'https://cdn.plot.ly/plotly-1.49.2.min'}}) || require;\n",
              "    xplotRequire(['plotly'], function(Plotly) { \r\n",
              "\n",
              "            var data = [{\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[3561386.8891544966,3042411.809392477,470533.78423206054,213204.6418280457,23232711.288128372,220476.83927746155,27456801.621234432,491637.0305007398,3931527.5392462616,326169.33068165416,257524.2529693309,357661.55704298755,525960.5558755693,365322.7743054506,258526.79187412205,259975.12511804298,216371.12018655136,213142.46709352545,201021.0736768141,190583.84976514935,253756.30131051247,492614.18474871706,272971.49887710775,141070.23920263263,8483237.087345976,161282.50610741967,189358.21714970074,1366093.4570276293,164262.12035902127,178038.64167897656,133755.73485829867,173863.88211638012,125358.09550687473,112671.19333906026,191868.19024758757,23925700.12174178,29822129.34606474,91361.05253250411,138310.598461552,29856769.768090684,3853227.4174370626,170534.13011513927],\"mode\":\"markers\"}];\n",
              "           var layout = {\"title\":\"Plot metrics over trials.\",\"showlegend\":false,\"xaxis\":{\"title\":\"Trial\",\"_isSubplotObj\":true},\"yaxis\":{\"title\":\"Metric\",\"_isSubplotObj\":true}};\n",
              "           Plotly.newPlot('1b600c5e-baef-44ed-8d2e-6f314fbc7cba', data, layout);\n",
              "        \r\n",
              "});\n",
              "};\r\n",
              "// ensure `require` is available globally\r\n",
              "if ((typeof(require) !==  typeof(Function)) || (typeof(require.config) !== typeof(Function))) {\r\n",
              "    let require_script = document.createElement('script');\r\n",
              "    require_script.setAttribute('src', 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js');\r\n",
              "    require_script.setAttribute('type', 'text/javascript');\r\n",
              "    \r\n",
              "    \r\n",
              "    require_script.onload = function() {\r\n",
              "        renderPlotly();\r\n",
              "    };\r\n",
              "\r\n",
              "    document.getElementsByTagName('head')[0].appendChild(require_script);\r\n",
              "}\r\n",
              "else {\r\n",
              "    renderPlotly();\r\n",
              "}\r\n",
              "\r\n",
              "</script>\r\n",
              "<table id=\"table_637891712613162135\"><caption><h3 style=\"text-align: center;\">DataFrame - 42 rows </h3></caption><thead><tr><th><i>index</i></th><th>Trial</th><th>Metric</th><th>Pipeline</th></tr></thead><tbody><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">0</div></td><td><div class=\"dni-plaintext\">3561387</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">1</div></td><td><div class=\"dni-plaintext\">3042411.8</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">2</div></td><td><div class=\"dni-plaintext\">470533.78</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">3</div></td><td><div class=\"dni-plaintext\">213204.64</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">4</div></td><td><div class=\"dni-plaintext\">23232712</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">5</div></td><td><div class=\"dni-plaintext\">220476.84</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">6</div></td><td><div class=\"dni-plaintext\">27456802</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">7</div></td><td><div class=\"dni-plaintext\">491637.03</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">8</div></td><td><div class=\"dni-plaintext\">3931527.5</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">9</div></td><td><div class=\"dni-plaintext\">326169.34</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">10</div></i></td><td><div class=\"dni-plaintext\">10</div></td><td><div class=\"dni-plaintext\">257524.25</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">11</div></i></td><td><div class=\"dni-plaintext\">11</div></td><td><div class=\"dni-plaintext\">357661.56</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">12</div></i></td><td><div class=\"dni-plaintext\">12</div></td><td><div class=\"dni-plaintext\">525960.56</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">13</div></i></td><td><div class=\"dni-plaintext\">13</div></td><td><div class=\"dni-plaintext\">365322.78</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">14</div></i></td><td><div class=\"dni-plaintext\">14</div></td><td><div class=\"dni-plaintext\">258526.8</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">15</div></i></td><td><div class=\"dni-plaintext\">15</div></td><td><div class=\"dni-plaintext\">259975.12</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">16</div></i></td><td><div class=\"dni-plaintext\">16</div></td><td><div class=\"dni-plaintext\">216371.12</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">17</div></i></td><td><div class=\"dni-plaintext\">17</div></td><td><div class=\"dni-plaintext\">213142.47</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">18</div></i></td><td><div class=\"dni-plaintext\">18</div></td><td><div class=\"dni-plaintext\">201021.08</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">19</div></i></td><td><div class=\"dni-plaintext\">19</div></td><td><div class=\"dni-plaintext\">190583.84</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">20</div></i></td><td><div class=\"dni-plaintext\">20</div></td><td><div class=\"dni-plaintext\">253756.3</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">21</div></i></td><td><div class=\"dni-plaintext\">21</div></td><td><div class=\"dni-plaintext\">492614.2</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">22</div></i></td><td><div class=\"dni-plaintext\">22</div></td><td><div class=\"dni-plaintext\">272971.5</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">23</div></i></td><td><div class=\"dni-plaintext\">23</div></td><td><div class=\"dni-plaintext\">141070.23</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">24</div></i></td><td><div class=\"dni-plaintext\">24</div></td><td><div class=\"dni-plaintext\">8483237</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">25</div></i></td><td><div class=\"dni-plaintext\">25</div></td><td><div class=\"dni-plaintext\">161282.5</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">26</div></i></td><td><div class=\"dni-plaintext\">26</div></td><td><div class=\"dni-plaintext\">189358.22</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">27</div></i></td><td><div class=\"dni-plaintext\">27</div></td><td><div class=\"dni-plaintext\">1366093.5</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">28</div></i></td><td><div class=\"dni-plaintext\">28</div></td><td><div class=\"dni-plaintext\">164262.12</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">29</div></i></td><td><div class=\"dni-plaintext\">29</div></td><td><div class=\"dni-plaintext\">178038.64</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">30</div></i></td><td><div class=\"dni-plaintext\">30</div></td><td><div class=\"dni-plaintext\">133755.73</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">31</div></i></td><td><div class=\"dni-plaintext\">31</div></td><td><div class=\"dni-plaintext\">173863.88</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">32</div></i></td><td><div class=\"dni-plaintext\">32</div></td><td><div class=\"dni-plaintext\">125358.09</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">33</div></i></td><td><div class=\"dni-plaintext\">33</div></td><td><div class=\"dni-plaintext\">112671.195</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">34</div></i></td><td><div class=\"dni-plaintext\">34</div></td><td><div class=\"dni-plaintext\">191868.19</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">35</div></i></td><td><div class=\"dni-plaintext\">35</div></td><td><div class=\"dni-plaintext\">23925700</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">36</div></i></td><td><div class=\"dni-plaintext\">36</div></td><td><div class=\"dni-plaintext\">29822130</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">37</div></i></td><td><div class=\"dni-plaintext\">37</div></td><td><div class=\"dni-plaintext\">91361.055</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">38</div></i></td><td><div class=\"dni-plaintext\">38</div></td><td><div class=\"dni-plaintext\">138310.6</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">39</div></i></td><td><div class=\"dni-plaintext\">39</div></td><td><div class=\"dni-plaintext\">29856770</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">40</div></i></td><td><div class=\"dni-plaintext\">40</div></td><td><div class=\"dni-plaintext\">3853227.5</div></td><td>Unknown=&gt;LightGbmRegression</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">41</div></i></td><td><div class=\"dni-plaintext\">41</div></td><td><div class=\"dni-plaintext\">170534.12</div></td><td>Unknown=&gt;LightGbmRegression</td></tr></tbody><tfoot><tr><td colspan=\"4\" style=\"text-align: center;\"><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = 1; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏮</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(-10); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏪</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(-1); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">◀️</button><b style=\"margin: 2px;\">Page</b><b id=\"page_637891712613162135\" style=\"margin: 2px;\">1</b><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(1); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">▶️</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(10); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏩</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637891712613162135 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML = 2; var page = parseInt(document.querySelector(&#39;#page_637891712613162135&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏭️</button></td></tr></tfoot></table><script>var page = parseInt(document.querySelector('#page_637891712613162135').innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637891712613162135 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j < 25; j++) { pageRows[j].style.display='table-row'; } </script>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var pipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "    .Append(context.Auto().Regression(\"y\", useFastForest: false, useFastTree: false, useLbfgs: false, useSdca: false));\n",
        "\n",
        "var monitor = new NotebookMonitor();\n",
        "\n",
        "var experiment = context.Auto().CreateExperiment();\n",
        "experiment.SetPipeline(pipeline)\n",
        "        .SetEvaluateMetric(RegressionMetric.RootMeanSquaredError, \"y\")\n",
        "        .SetTrainingTimeInSeconds(120)\n",
        "        .SetDataset(trainTestSplit.TrainSet, trainTestSplit.TestSet)\n",
        "        .SetMonitor(monitor);\n",
        "\n",
        "// Configure Visualizer\t\t\t\n",
        "monitor.SetUpdate(monitor.Display());\n",
        "\n",
        "var res = await experiment.Run();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check model/metric from experiment result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"dni-plaintext\">91361.05253250411</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// get model\n",
        "var model  = res.Model;\n",
        "var eval = model.Transform(trainTestSplit.TestSet);\n",
        "var metric = context.Regression.Evaluate(eval, \"y\");\n",
        "\n",
        "// should be identical with res.Metric\n",
        "metric.RootMeanSquaredError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Continue learning\n",
        "\n",
        "> [⏩ Next Module - Model Evaluation](https://raw.githubusercontent.com/JakeRadMSFT/csharp-notebooks/main/machine-learning/04-Model%20Evaluation.ipynb)  \n",
        "> [⏪ Last Module - Data Prep and Feature Engineering](https://raw.githubusercontent.com/JakeRadMSFT/csharp-notebooks/main/machine-learning/02-Data%20Preparation%20and%20Feature%20Engineering.ipynb)  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".NET (C#)",
      "language": "C#",
      "name": ".net-csharp"
    },
    "language_info": {
      "file_extension": ".cs",
      "mimetype": "text/x-csharp",
      "name": "C#",
      "pygments_lexer": "csharp",
      "version": "8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
