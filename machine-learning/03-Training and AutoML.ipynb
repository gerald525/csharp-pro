{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In this Notebook, you will learn\n",
        "- What does __Training__ mean?\n",
        "- Introduction to trainers,  some of their differences, and how to decide which one to use.\n",
        "- How hyper-parameters impact training performance.\n",
        "- How to use AutoML to simplify your training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What does __Training__ mean.\n",
        "Before diving into code, let's first talk a little about what does \"train a model\" actually mean. \n",
        "\n",
        "In ML.Net, \"train a model\" usually means call `model.Fit(X)` in ML.Net, where `X` is an `IDataView` which includes both feature and label. So what happen when you call `Fit`? Generally speaking, `Fit` updates parameters in the trainer so it can predict label that is **close** to the actual label in `X`, or in another word, to decrease the distance between predicted and actual label.\n",
        "\n",
        "In machine learning, the difference or distance between predicted and actual label is usually called **loss** and you use different loss measures based on the task. For classification softmax is a common loss measure. For regression, Root Mean Squared Error (RMSE) is a common loss measure. In general though, they are all metrics to quantify the distance between the predicted and actual label. In most of cases, a **lower loss means a better model**. For more information, see the [ML.NET evaluation metrics guide](https://docs.microsoft.com/dotnet/machine-learning/resources/metrics).\n",
        "\n",
        "So what `Fit` does is to apply an algorithm to your data to identify patterns and adjust parameters in that algorithm to lowers the loss. When you train a model, you want to decrease its loss to make the prediction of that model closer to the actual label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainers in ML.Net\n",
        "ML.NET provides a variety of trainers. You can find most of them under the [StandardTrainersCatalog](https://docs.microsoft.com/dotnet/api/microsoft.ml.standardtrainerscatalog?view=ml-dotnet). Examples of trainers include linear trainers like `SDCA`, `Lbfgs`, `LinearSvm` and tree-based non-linear trainers like `FastTree`, `RandomForest` and `LightGbm`. Generally, each trainer's capability is different. Non-linear models sometimes have better training performance (lower loss) than linear ones, but it doesn't always mean they are always the better choice. Picking the right trainer to build the best model for your data requires many attempts of trial and error.\n",
        "\n",
        "### Overfitting and Underfitting\n",
        "Overfitting and underfitting are the two most common problems you encounter when training a model. Underfitting means the selected trainer is not capable enough to fit training dataset and usually result in a high loss during training and low score/metric on test dataset. To resolve this you need to either select a more powerful model or perform more feature engineering. Overfitting is the opposite, which happens when model learns the training data too well. This usually results in low loss metric during training but high loss on test dataset.\n",
        "\n",
        "A good analogy for these concepts is studying for an exam. Let's say you knew the questions and answers ahead of time. After studying, you take the test and get a perfect score. Great news! However, when you're given the exam again with the questions rearranged and with slightly different wording you get a lower score. That suggests you memorized the answers and didn't actually learn the concepts you were being tested on. This is an example of overfitting. Underfitting is the opposite where the study materials you were given don't accurately represent what you're evaluated on for the exam. As a result, you resort to guessing the answers since you don't have enough knowledge to answer correctly.\n",
        "\n",
        "### Difference in parameter and hyper-parameter\n",
        "In the nutshell, parameters are internal to a trainer, and is updated based on training dataset during training(`Fit`) process. While hyper-parameters are external to a trainer and control training process. For example, in `LightGbm`, `LearningRate` is a hyper-parameter which you can designate when creating and it controls the updating steps for the tree nodes weight during training. And tree nodes weight is parameter which is adjusted during `Fit` process.\n",
        "\n",
        "### Hyper-parameter optimization\n",
        "Choosing the right trainer impacts your final training performance. Choosing the right hyper-parameters also has a huge impact over the final training performance, especially for tree-base trainers. Hyper-parameters are important because it controls how parameter being updated. For example, larger `numberOfLeaves` in `LightGbm` produces a larger model and usually enables it to fit on a more complex dataset, but it might have countereffect on small dataset and cause **overfitting**. Conversely, if the dataset is complex but you set a small `numberOfLeaves`, it might impair `LightGbm`'s ability on fitting that dataset and cause **underfit**.\n",
        "\n",
        "The process of finding the best configuration for your trainer is known as hyper-parameter optimization (HPO). Like the process of choosing your trainer it involves a lot of trial and error. The built-in Automated ML (AutoML) capabilities in ML.NET simplify the HPO process.\n",
        "\n",
        "In the next section, we will go through two examples. The first example trains a regression model on a linear dataset using both linear and more advanced non-linear trainers to highlight the importance of selecting the right trainer. The second example trains a regression model on a non-linear dataset using `LightGbm` with different hyper-parameters to show the importance of hyper-parameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Linear regression\n",
        "In the below section, we are going to show the difference of trainers via a linear regression task. First, we fit the linear dataset with the linear trainer, `SDCA`. Then we git the linear dataset with `LightGbm`, a tree-base non-linear trainer. Their performance is evaluated against a test dataset. The code below:\n",
        "- Creates a linear dataset and splits it into train/test sets\n",
        "- Create training pipelines using `SDCA` and `LightGbm`\n",
        "- Trains both `SDCA` and `LightGbm` on the linear training set, and evaluates them on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><div><strong>Restore sources</strong><ul><li><span>https://mlnetcli.blob.core.windows.net/mlnetcli/index.json</span></li><li><span>https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json</span></li><li><span>https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet5/nuget/v3/index.json</span></li></ul></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Data.Analysis, 0.20.0-preview.22313.1</span></li><li><span>Microsoft.DotNet.Interactive.Formatting, 1.0.0-beta.22256.1</span></li><li><span>Microsoft.ML.AutoML, 0.20.0-preview.22313.1</span></li><li><span>MLNetAutoML.InteractiveExtension, 0.2.0</span></li><li><span>XPlot.Plotly.Interactive, 4.0.6</span></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `XPlot.Plotly.Interactive.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Configuring PowerShell Kernel for XPlot.Plotly integration."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Installed support for XPlot.Plotly."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `MLNetAutoML.InteractiveExtension.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Loading extensions from `Microsoft.Data.Analysis.Interactive.dll`"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// install dependencies and import using statement\n",
        "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet5/nuget/v3/index.json\" \n",
        "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json\"\n",
        "#i \"nuget:https://mlnetcli.blob.core.windows.net/mlnetcli/index.json\"\n",
        "\n",
        "#r \"nuget:MLNetAutoML.InteractiveExtension,0.2.0\"\n",
        "\n",
        "// XPlot is used to plot trials during training because it's strong-named. However it's recommended to use Ploty.Net to plot digrams in notebook.\n",
        "#r \"nuget:Microsoft.DotNet.Interactive.Formatting, 1.0.0-beta.22256.1\"\n",
        "#r \"nuget:XPlot.Plotly.Interactive,4.0.6\"\n",
        "#r \"nuget:Microsoft.ML.AutoML,0.20.0-preview.22313.1\"\n",
        "#r \"nuget:Microsoft.Data.Analysis,0.20.0-preview.22313.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "// Import usings.\n",
        "using Microsoft.Data.Analysis;\n",
        "using System;\n",
        "using System.IO;\n",
        "using Microsoft.ML;\n",
        "using Microsoft.ML.AutoML;\n",
        "using Microsoft.ML.Data;\n",
        "using MLNetAutoML.InteractiveExtension;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create linear dataset\n",
        "The code below creates a linear dataset with a random residual. The dataset is loaded into train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table id=\"table_637915799293597156\"><thead><tr><th><i>index</i></th><th>X</th><th>y</th></tr></thead><tbody><tr><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">-1000</div></td><td><div class=\"dni-plaintext\">-99997.734</div></td></tr><tr><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">-999.9</div></td><td><div class=\"dni-plaintext\">-99986.83</div></td></tr><tr><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">-999.8</div></td><td><div class=\"dni-plaintext\">-99977.32</div></td></tr><tr><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">-999.7</div></td><td><div class=\"dni-plaintext\">-99969.42</div></td></tr><tr><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">-999.60004</div></td><td><div class=\"dni-plaintext\">-99962.94</div></td></tr><tr><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">-999.5</div></td><td><div class=\"dni-plaintext\">-99949.414</div></td></tr><tr><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">-999.4</div></td><td><div class=\"dni-plaintext\">-99935.94</div></td></tr><tr><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">-999.3</div></td><td><div class=\"dni-plaintext\">-99930.58</div></td></tr><tr><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">-999.2</div></td><td><div class=\"dni-plaintext\">-99915.23</div></td></tr><tr><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">-999.10004</div></td><td><div class=\"dni-plaintext\">-99912.266</div></td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var rand = new Random(0);\n",
        "var context =new MLContext(seed: 1);\n",
        "var x = Enumerable.Range(-10000, 10000).Select(_x => _x * 0.1f).ToArray();\n",
        "var y = x.Select(_x => 100 * _x + (rand.NextSingle() - 0.5f) * 10).ToArray();\n",
        "var df = new DataFrame();\n",
        "df[\"X\"] = DataFrameColumn.Create(\"X\", x);\n",
        "df[\"y\"] = DataFrameColumn.Create(\"y\", y);\n",
        "var trainTestSplit = context.Data.TrainTestSplit(df);\n",
        "df.Head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct pipeline\n",
        "The code below shows how to construct training pipelines for both `SDCA` and `LightGbm`. The `Concatenate` transformer is required to convert a `single` column into `Vector<single>` type, which is the expected feature type for both `SDCA` and `LightGbm` regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var sdcaPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.Sdca(\"y\"));\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var lgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\"));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate model\n",
        "The code below first trains `sdcaPipeline` and `lgbmPipeline` which are created above, then evaluate their performance on test dataset by calculating `Root Mean Square Error` between predicted and actual value. `SDCA` has better performance with a significantly lower `Root Mean Square Error` compared to `LightGbm` even though it's a simpler linear model. This is because the training dataset is also linear, so `SDCA` can fit the dataset better than `LightGbm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdca rmse on trainset: 2.8893647679179795, lgbm rmse on trainset: 117.9591141671402\r\n",
            "sdca rmse on testset: 2.9233712054149024, lgbm rmse on testset: 119.860254032606\r\n"
          ]
        }
      ],
      "source": [
        "var sdcaModel = sdcaPipeline.Fit(trainTestSplit.TrainSet);\n",
        "var lgbmModel = lgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "\n",
        "// evaluate on train set\n",
        "var sdcaTrainEval = sdcaModel.Transform(trainTestSplit.TrainSet);\n",
        "var sdcaTrainMetric = context.Regression.Evaluate(sdcaTrainEval, \"y\");\n",
        "\n",
        "var lgbmTrainEval = lgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var lgbmTrainMetric = context.Regression.Evaluate(lgbmTrainEval, \"y\");\n",
        "\n",
        "Console.WriteLine($\"sdca rmse on trainset: {sdcaTrainMetric.RootMeanSquaredError}, lgbm rmse on trainset: {lgbmTrainMetric.RootMeanSquaredError}\");\n",
        "\n",
        "// evaluate on test set\n",
        "var sdcaTestEval = sdcaModel.Transform(trainTestSplit.TestSet);\n",
        "var sdcaTestMetric = context.Regression.Evaluate(sdcaTestEval, \"y\");\n",
        "\n",
        "var lgbmTestEval = lgbmModel.Transform(trainTestSplit.TestSet);\n",
        "var lgbmTestMetric = context.Regression.Evaluate(lgbmTestEval, \"y\");\n",
        "Console.WriteLine($\"sdca rmse on testset: {sdcaTestMetric.RootMeanSquaredError}, lgbm rmse on testset: {lgbmTestMetric.RootMeanSquaredError}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Non-linear regression on LightGbm.\n",
        "This example shows the importance of hyper-parameter optimization. First we create a non-linear dataset and two pipelines. One pipeline has `LightGbm` with `numberOfLeaves` set to `10`, the other's set to `1000`. Both pipelines are trained with the same training dataset and their training performance is evaluated on the same test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create non-linear dataset\n",
        "The code below creates a non-linear dataset with a random residual. The dataset is loaded into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table id=\"table_637915799416257866\"><thead><tr><th><i>index</i></th><th>X</th><th>y</th></tr></thead><tbody><tr><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">-1000</div></td><td><div class=\"dni-plaintext\">100000000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">-999.9</div></td><td><div class=\"dni-plaintext\">99980000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">-999.8</div></td><td><div class=\"dni-plaintext\">99960000</div></td></tr><tr><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">-999.7</div></td><td><div class=\"dni-plaintext\">99940010</div></td></tr><tr><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">-999.60004</div></td><td><div class=\"dni-plaintext\">99920020</div></td></tr><tr><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">-999.5</div></td><td><div class=\"dni-plaintext\">99900024</div></td></tr><tr><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">-999.4</div></td><td><div class=\"dni-plaintext\">99880050</div></td></tr><tr><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">-999.3</div></td><td><div class=\"dni-plaintext\">99860050</div></td></tr><tr><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">-999.2</div></td><td><div class=\"dni-plaintext\">99840070</div></td></tr><tr><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">-999.10004</div></td><td><div class=\"dni-plaintext\">99820090</div></td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var rand = new Random(0);\n",
        "var context =new MLContext(seed: 1);\n",
        "var x = Enumerable.Range(-10000, 10000).Select(_x => _x * 0.1f).ToArray();\n",
        "var y = x.Select(_x => 100 * _x * _x + (rand.NextSingle() - 0.5f) * 10).ToArray();\n",
        "var df = new DataFrame();\n",
        "df[\"X\"] = DataFrameColumn.Create(\"X\", x);\n",
        "df[\"y\"] = DataFrameColumn.Create(\"y\", y);\n",
        "var trainTestSplit = context.Data.TrainTestSplit(df);\n",
        "df.Head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct pipeline\n",
        "The code below shows how to construct training pipelines for `LightGbm` with different hyper-parameters. The `Concatenate` transformer is required because it converts a `single` column into `Vector<single>` type, which is the expected feature type for the `LightGbm` trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var smallLgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\", numberOfLeaves: 10));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [],
      "source": [
        "var largeLgbmPipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "                            .Append(context.Regression.Trainers.LightGbm(\"y\", numberOfLeaves: 1000));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate model\n",
        "The code below first trains `smallLgbmPipeline` and `largeLgbmPipeline` which are created above, then evaluates their performance on the test dataset by calculating the `Root Mean Square Error` between predicted and actual value. The model created by `largeLgbmPipeline` has better performance with a lower RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "small lgbm rmse on testset: 173938.52924678137, large lgbm rmse on testset: 132927.2510939994\r\n"
          ]
        }
      ],
      "source": [
        "var smallLgbmModel = smallLgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "var largeLgbmModel = largeLgbmPipeline.Fit(trainTestSplit.TrainSet);\n",
        "\n",
        "// evaluate on test set\n",
        "var smallTestEval = smallLgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var smallLgbmMetric = context.Regression.Evaluate(smallTestEval, \"y\");\n",
        "\n",
        "var largeLgbmEval = largeLgbmModel.Transform(trainTestSplit.TrainSet);\n",
        "var largeLgbmMetric = context.Regression.Evaluate(largeLgbmEval, \"y\");\n",
        "\n",
        "Console.WriteLine($\"small lgbm rmse on testset: {smallLgbmMetric.RootMeanSquaredError}, large lgbm rmse on testset: {largeLgbmMetric.RootMeanSquaredError}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use AutoML to simplify trainer selection and hyper-parameter optimization.\n",
        "Trainer selection and Hyper-parameter optimization is an important process with lots of trial and error. This process can be automated and simplified using the built-in `AutoMLExperiment`. `AutoMLExperiment` applies the latest research from Microsoft Research to conduct a swift, accurate and thorough hyper-parameter optimization given a limited time budget.\n",
        "\n",
        "The code below shows how to use `AutoMLExperiment` for finding the best trainer along with its best hyper parrameter on the non-linear dataset used in Example 2. Firstly, a `SweepableEstimatorPipeline` is created via `context.Auto().Regression(\"y\")`, which returns the most popular regressors along with their default search space in ML.Net. Then an `AutoMLExperiment` is created. It uses `RootMeanSquaredError` as optimization metric and train-test validation to evaluate trial score, and uses `NotebookMonitor` to present training process. Once the training is completed, it returns the best trial as result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><h3>Best Run</h3><p>Trial: 22</p><p>Trainer: FastTreeRegression</p><h3>Active Run</h3><p>Trial: 42</p><p>Trainer: FastTreeRegression</p><p>Parameters: {\r\n",
              "  &quot;0&quot;: {},\r\n",
              "  &quot;1&quot;: {\r\n",
              "    &quot;NumberOfLeaves&quot;: 57,\r\n",
              "    &quot;MinimumExampleCountPerLeaf&quot;: 39,\r\n",
              "    &quot;NumberOfTrees&quot;: 13527,\r\n",
              "    &quot;MaximumBinCountPerFeature&quot;: 943,\r\n",
              "    &quot;FeatureFraction&quot;: 0.99999999,\r\n",
              "    &quot;LearningRate&quot;: 0.9999997766729865,\r\n",
              "    &quot;LabelColumnName&quot;: &quot;y&quot;,\r\n",
              "    &quot;FeatureColumnName&quot;: &quot;Features&quot;\r\n",
              "  }\r\n",
              "}</p></div><!DOCTYPE html>\r\n",
              "<div style=\"width: 500px; height: 500px;\" id=\"5cbc8742-2deb-4c33-a04d-6f2e4fb75064\"></div><script type=\"text/javascript\">\r\n",
              "\n",
              "var renderPlotly = function() {\n",
              "    var xplotRequire = require.config({context:'xplot-3.0.1',paths:{plotly:'https://cdn.plot.ly/plotly-1.49.2.min'}}) || require;\n",
              "    xplotRequire(['plotly'], function(Plotly) { \r\n",
              "\n",
              "            var data = [{\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[30531504.718025338,21401976.13975807,3561386.8891544966,3174951.2975316527,2547693.1699879905,25619227.521369632,911643.4209936514,3547802.17673204,938492.7720823191,28120465.90975749,39783834.46114094,443821.61278409243,153158.60908242912,440951.2953051753,8142317.121008107,1465820.0957857524,106855.67535736138,660824.1836249065,85469.89113455005,916905.3349561897,100800.01193879842,21145849.486888506,38236.134331302615,518091.98343516415,45823904.27701145,17247156.780360937,130788.07431799818,627215.4275066127,38242.86512398753,79159.10723701552,45823904.204411164,40561.94078930606,45823645.85866439,4960165.508601603,63520.44287262229,3293924.4763792995,55926.51219760892,30307958.621631496,31730313.871344764,128906.47693182451,2824187.7381594954,41229.85612764947],\"mode\":\"markers\"}];\n",
              "           var layout = {\"title\":\"Plot metrics over trials.\",\"showlegend\":false,\"xaxis\":{\"title\":\"Trial\",\"_isSubplotObj\":true},\"yaxis\":{\"title\":\"Metric\",\"_isSubplotObj\":true}};\n",
              "           Plotly.newPlot('5cbc8742-2deb-4c33-a04d-6f2e4fb75064', data, layout);\n",
              "        \r\n",
              "});\n",
              "};\r\n",
              "// ensure `require` is available globally\r\n",
              "if ((typeof(require) !==  typeof(Function)) || (typeof(require.config) !== typeof(Function))) {\r\n",
              "    let require_script = document.createElement('script');\r\n",
              "    require_script.setAttribute('src', 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js');\r\n",
              "    require_script.setAttribute('type', 'text/javascript');\r\n",
              "    \r\n",
              "    \r\n",
              "    require_script.onload = function() {\r\n",
              "        renderPlotly();\r\n",
              "    };\r\n",
              "\r\n",
              "    document.getElementsByTagName('head')[0].appendChild(require_script);\r\n",
              "}\r\n",
              "else {\r\n",
              "    renderPlotly();\r\n",
              "}\r\n",
              "\r\n",
              "</script>\r\n",
              "<table id=\"table_637915802291492633\"><caption><h3 style=\"text-align: center;\">DataFrame - 42 rows </h3></caption><thead><tr><th><i>index</i></th><th>Trial</th><th>Metric</th><th>Trainer</th><th>Parameters</th></tr></thead><tbody><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">0</div></i></td><td><div class=\"dni-plaintext\">0</div></td><td><div class=\"dni-plaintext\">30531504</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;NumberOfTrees&quot;:4,&quot;MaximumBinCountPerFeature&quot;:255,&quot;FeatureFraction&quot;:1,&quot;LearningRate&quot;:0.09999999999999998,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">1</div></i></td><td><div class=\"dni-plaintext\">1</div></td><td><div class=\"dni-plaintext\">21401976</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;NumberOfTrees&quot;:11,&quot;MaximumBinCountPerFeature&quot;:331,&quot;FeatureFraction&quot;:0.9546183659121279,&quot;LearningRate&quot;:0.07607640468668345,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">2</div></i></td><td><div class=\"dni-plaintext\">2</div></td><td><div class=\"dni-plaintext\">3561387</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:1,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:1,&quot;MaximumBinCountPerFeature&quot;:255,&quot;FeatureFraction&quot;:1,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:1,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">3</div></i></td><td><div class=\"dni-plaintext\">3</div></td><td><div class=\"dni-plaintext\">3174951.2</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.6748491991079117,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:316,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:3.0649092157264173E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">4</div></i></td><td><div class=\"dni-plaintext\">4</div></td><td><div class=\"dni-plaintext\">2547693.2</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:21,&quot;LearningRate&quot;:0.7968094557309192,&quot;NumberOfTrees&quot;:6,&quot;SubsampleFraction&quot;:0.16941334755092521,&quot;MaximumBinCountPerFeature&quot;:365,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:3.1835150143181827E-09,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">5</div></i></td><td><div class=\"dni-plaintext\">5</div></td><td><div class=\"dni-plaintext\">25619228</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.01117388908996919,&quot;NumberOfTrees&quot;:16,&quot;SubsampleFraction&quot;:0.09324656947243495,&quot;MaximumBinCountPerFeature&quot;:424,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.14451834597760635,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">6</div></i></td><td><div class=\"dni-plaintext\">6</div></td><td><div class=\"dni-plaintext\">911643.44</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:28,&quot;NumberOfTrees&quot;:35,&quot;MaximumBinCountPerFeature&quot;:208,&quot;FeatureFraction&quot;:0.8696602453046111,&quot;LearningRate&quot;:0.32733048622278765,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">7</div></i></td><td><div class=\"dni-plaintext\">7</div></td><td><div class=\"dni-plaintext\">3547802.2</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:33,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:0.30779558423213604,&quot;MaximumBinCountPerFeature&quot;:314,&quot;FeatureFraction&quot;:0.9117732436510455,&quot;L1Regularization&quot;:8.189795261959435E-08,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">8</div></i></td><td><div class=\"dni-plaintext\">8</div></td><td><div class=\"dni-plaintext\">938492.75</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:37,&quot;NumberOfTrees&quot;:27,&quot;MaximumBinCountPerFeature&quot;:261,&quot;FeatureFraction&quot;:0.7565912972965214,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">9</div></i></td><td><div class=\"dni-plaintext\">9</div></td><td><div class=\"dni-plaintext\">28120466</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.019675783848973453,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:603,&quot;FeatureFraction&quot;:0.9268829692688172,&quot;L1Regularization&quot;:5.512683270508932E-08,&quot;L2Regularization&quot;:0.9428056937756522,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">10</div></i></td><td><div class=\"dni-plaintext\">10</div></td><td><div class=\"dni-plaintext\">39783836</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:23,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;NumberOfTrees&quot;:43,&quot;MaximumBinCountPerFeature&quot;:166,&quot;FeatureFraction&quot;:0.9827291933127007,&quot;LearningRate&quot;:0.0033642933171972983,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">11</div></i></td><td><div class=\"dni-plaintext\">11</div></td><td><div class=\"dni-plaintext\">443821.62</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:76,&quot;NumberOfTrees&quot;:66,&quot;MaximumBinCountPerFeature&quot;:417,&quot;FeatureFraction&quot;:0.9978006800924352,&quot;LearningRate&quot;:0.37571946443752924,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">12</div></i></td><td><div class=\"dni-plaintext\">12</div></td><td><div class=\"dni-plaintext\">153158.61</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:16,&quot;NumberOfTrees&quot;:142,&quot;MaximumBinCountPerFeature&quot;:424,&quot;FeatureFraction&quot;:0.9991459417216926,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">13</div></i></td><td><div class=\"dni-plaintext\">13</div></td><td><div class=\"dni-plaintext\">440951.28</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:5,&quot;MinimumExampleCountPerLeaf&quot;:21,&quot;NumberOfTrees&quot;:75,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.46666701715693565,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">14</div></i></td><td><div class=\"dni-plaintext\">14</div></td><td><div class=\"dni-plaintext\">8142317</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:11,&quot;NumberOfTrees&quot;:271,&quot;MaximumBinCountPerFeature&quot;:57,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.006688212836740798,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">15</div></i></td><td><div class=\"dni-plaintext\">15</div></td><td><div class=\"dni-plaintext\">1465820.1</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:24,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:12,&quot;SubsampleFraction&quot;:0.0028978218248112433,&quot;MaximumBinCountPerFeature&quot;:220,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">16</div></i></td><td><div class=\"dni-plaintext\">16</div></td><td><div class=\"dni-plaintext\">106855.67</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:36,&quot;MinimumExampleCountPerLeaf&quot;:65,&quot;NumberOfTrees&quot;:9234,&quot;MaximumBinCountPerFeature&quot;:369,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">17</div></i></td><td><div class=\"dni-plaintext\">17</div></td><td><div class=\"dni-plaintext\">660824.2</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:6,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:17,&quot;SubsampleFraction&quot;:0.10837382077485433,&quot;MaximumBinCountPerFeature&quot;:752,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:7.520796569912086E-09,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">18</div></i></td><td><div class=\"dni-plaintext\">18</div></td><td><div class=\"dni-plaintext\">85469.89</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:41,&quot;MinimumExampleCountPerLeaf&quot;:58,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:146,&quot;SubsampleFraction&quot;:0.5094107373203106,&quot;MaximumBinCountPerFeature&quot;:534,&quot;FeatureFraction&quot;:0.5435991472853738,&quot;L1Regularization&quot;:6.068309983133183E-10,&quot;L2Regularization&quot;:0.0008811861604820598,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">19</div></i></td><td><div class=\"dni-plaintext\">19</div></td><td><div class=\"dni-plaintext\">916905.3</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:19,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:127,&quot;FeatureFraction&quot;:0.99999999,&quot;L1Regularization&quot;:1.5012987092075794E-07,&quot;L2Regularization&quot;:7.237049641254363E-05,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">20</div></i></td><td><div class=\"dni-plaintext\">20</div></td><td><div class=\"dni-plaintext\">100800.016</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:36,&quot;MinimumExampleCountPerLeaf&quot;:126,&quot;NumberOfTrees&quot;:144,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.06785059497358972,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">21</div></i></td><td><div class=\"dni-plaintext\">21</div></td><td><div class=\"dni-plaintext\">21145850</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4519,&quot;MinimumExampleCountPerLeaf&quot;:216,&quot;LearningRate&quot;:0.0003415839852699775,&quot;NumberOfTrees&quot;:1055,&quot;SubsampleFraction&quot;:0.00014772939574951335,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:2E-10,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.01072935917143342,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">22</div></i></td><td><div class=\"dni-plaintext\">22</div></td><td><div class=\"dni-plaintext\">38236.133</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:29,&quot;MinimumExampleCountPerLeaf&quot;:21,&quot;NumberOfTrees&quot;:501,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">23</div></i></td><td><div class=\"dni-plaintext\">23</div></td><td><div class=\"dni-plaintext\">518091.97</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:389,&quot;MinimumExampleCountPerLeaf&quot;:375,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:278,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:516,&quot;FeatureFraction&quot;:0.8080319914785552,&quot;L1Regularization&quot;:0.00036889767711519153,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">24</div></i></td><td><div class=\"dni-plaintext\">24</div></td><td><div class=\"dni-plaintext\">45823904</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:211,&quot;MinimumExampleCountPerLeaf&quot;:2,&quot;NumberOfTrees&quot;:4,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.38948848574172534,&quot;LearningRate&quot;:0.5423885550675092,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">25</div></i></td><td><div class=\"dni-plaintext\">25</div></td><td><div class=\"dni-plaintext\">17247156</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.00799244246605245,&quot;NumberOfTrees&quot;:76,&quot;SubsampleFraction&quot;:0.03212996284294298,&quot;MaximumBinCountPerFeature&quot;:551,&quot;FeatureFraction&quot;:0.2791663030921924,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:1.0551333452750034E-09,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">26</div></i></td><td><div class=\"dni-plaintext\">26</div></td><td><div class=\"dni-plaintext\">130788.08</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:126,&quot;NumberOfTrees&quot;:32767,&quot;MaximumBinCountPerFeature&quot;:593,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">27</div></i></td><td><div class=\"dni-plaintext\">27</div></td><td><div class=\"dni-plaintext\">627215.44</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:85,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:25,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:613,&quot;FeatureFraction&quot;:0.49489629299106225,&quot;L1Regularization&quot;:4.6020020905138E-06,&quot;L2Regularization&quot;:1.7918425126974676E-07,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">28</div></i></td><td><div class=\"dni-plaintext\">28</div></td><td><div class=\"dni-plaintext\">38242.863</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:98,&quot;MinimumExampleCountPerLeaf&quot;:2,&quot;NumberOfTrees&quot;:20889,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">29</div></i></td><td><div class=\"dni-plaintext\">29</div></td><td><div class=\"dni-plaintext\">79159.11</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:620,&quot;MinimumExampleCountPerLeaf&quot;:38,&quot;LearningRate&quot;:0.4634531682150773,&quot;NumberOfTrees&quot;:836,&quot;SubsampleFraction&quot;:0.001715642899607758,&quot;MaximumBinCountPerFeature&quot;:464,&quot;FeatureFraction&quot;:0.5923020015796854,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">30</div></i></td><td><div class=\"dni-plaintext\">30</div></td><td><div class=\"dni-plaintext\">45823904</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:7,&quot;MinimumExampleCountPerLeaf&quot;:126,&quot;NumberOfTrees&quot;:12,&quot;MaximumBinCountPerFeature&quot;:117,&quot;FeatureFraction&quot;:0.7907469530677912,&quot;LearningRate&quot;:2E-10,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">31</div></i></td><td><div class=\"dni-plaintext\">31</div></td><td><div class=\"dni-plaintext\">40561.94</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:123,&quot;MinimumExampleCountPerLeaf&quot;:23,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:252,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.4812909943027274,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">32</div></i></td><td><div class=\"dni-plaintext\">32</div></td><td><div class=\"dni-plaintext\">45823644</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:6,&quot;MinimumExampleCountPerLeaf&quot;:43,&quot;NumberOfTrees&quot;:2310,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.9454546552788192,&quot;LearningRate&quot;:2.625577431723482E-09,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">33</div></i></td><td><div class=\"dni-plaintext\">33</div></td><td><div class=\"dni-plaintext\">4960165.5</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:25,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:5.549259654091752E-05,&quot;NumberOfTrees&quot;:32767,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.5157965930549329,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">34</div></i></td><td><div class=\"dni-plaintext\">34</div></td><td><div class=\"dni-plaintext\">63520.44</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:130,&quot;MinimumExampleCountPerLeaf&quot;:10,&quot;NumberOfTrees&quot;:108,&quot;MaximumBinCountPerFeature&quot;:555,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">35</div></i></td><td><div class=\"dni-plaintext\">35</div></td><td><div class=\"dni-plaintext\">3293924.5</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:590,&quot;MinimumExampleCountPerLeaf&quot;:881,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:0.009018365503931551,&quot;MaximumBinCountPerFeature&quot;:98,&quot;FeatureFraction&quot;:0.446785395550522,&quot;L1Regularization&quot;:7.779190710769625E-06,&quot;L2Regularization&quot;:0.002059783627236415,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">36</div></i></td><td><div class=\"dni-plaintext\">36</div></td><td><div class=\"dni-plaintext\">55926.51</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:2180,&quot;MinimumExampleCountPerLeaf&quot;:27,&quot;NumberOfTrees&quot;:176,&quot;MaximumBinCountPerFeature&quot;:651,&quot;FeatureFraction&quot;:0.99999999,&quot;LearningRate&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">37</div></i></td><td><div class=\"dni-plaintext\">37</div></td><td><div class=\"dni-plaintext\">30307958</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:32767,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:6.07802719498229E-06,&quot;NumberOfTrees&quot;:4,&quot;SubsampleFraction&quot;:4.444329155771778E-05,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.29937631876084847,&quot;L1Regularization&quot;:2.572288151773259E-10,&quot;L2Regularization&quot;:2.771033480958943E-07,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">38</div></i></td><td><div class=\"dni-plaintext\">38</div></td><td><div class=\"dni-plaintext\">31730314</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:15,&quot;NumberOfTrees&quot;:1413,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.9228184120268582,&quot;LearningRate&quot;:0.0002903320632447188,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">39</div></i></td><td><div class=\"dni-plaintext\">39</div></td><td><div class=\"dni-plaintext\">128906.48</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:4,&quot;MinimumExampleCountPerLeaf&quot;:119,&quot;LearningRate&quot;:0.9999997766729865,&quot;NumberOfTrees&quot;:23842,&quot;SubsampleFraction&quot;:0.9999997766729865,&quot;MaximumBinCountPerFeature&quot;:387,&quot;FeatureFraction&quot;:0.6632056698446064,&quot;L1Regularization&quot;:2E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">40</div></i></td><td><div class=\"dni-plaintext\">40</div></td><td><div class=\"dni-plaintext\">2824187.8</div></td><td>FastTreeRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:13,&quot;MinimumExampleCountPerLeaf&quot;:11,&quot;NumberOfTrees&quot;:18,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:0.9240566407354379,&quot;LearningRate&quot;:0.15179121634311543,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr><tr style=\"display: none\"><td><i><div class=\"dni-plaintext\">41</div></i></td><td><div class=\"dni-plaintext\">41</div></td><td><div class=\"dni-plaintext\">41229.855</div></td><td>LightGbmRegression</td><td>{&quot;0&quot;:{},&quot;1&quot;:{&quot;NumberOfLeaves&quot;:73,&quot;MinimumExampleCountPerLeaf&quot;:20,&quot;LearningRate&quot;:0.10355302561595342,&quot;NumberOfTrees&quot;:666,&quot;SubsampleFraction&quot;:9.671307875704858E-05,&quot;MaximumBinCountPerFeature&quot;:1022,&quot;FeatureFraction&quot;:2E-10,&quot;L1Regularization&quot;:4.541873418521388E-10,&quot;L2Regularization&quot;:0.9999997766729865,&quot;LabelColumnName&quot;:&quot;y&quot;,&quot;FeatureColumnName&quot;:&quot;Features&quot;}}</td></tr></tbody><tfoot><tr><td colspan=\"5\" style=\"text-align: center;\"><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = 1; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏮</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(-10); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏪</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(-1); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">◀️</button><b style=\"margin: 2px;\">Page</b><b id=\"page_637915802291492633\" style=\"margin: 2px;\">1</b><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(1); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">▶️</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; page = parseInt(page) + parseInt(10); page = page &lt; 0 ? 0 : page; page = page > 1 ? 1 : page; document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = page + 1; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏩</button><button style=\"margin: 2px;\" onclick=\"var allRows = document.querySelectorAll(&#39;#table_637915802291492633 tbody tr:nth-child(n)&#39;); for (let i = 0; i &lt; allRows.length; i++) { allRows[i].style.display=&#39;none&#39;; } document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML = 2; var page = parseInt(document.querySelector(&#39;#page_637915802291492633&#39;).innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j &lt; 25; j++) { pageRows[j].style.display=&#39;table-row&#39;; } \">⏭️</button></td></tr></tfoot></table><script>var page = parseInt(document.querySelector('#page_637915802291492633').innerHTML) - 1; var pageRows = document.querySelectorAll(`#table_637915802291492633 tbody tr:nth-child(n + ${page * 25 + 1 })`); for (let j = 0; j < 25; j++) { pageRows[j].style.display='table-row'; } </script>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "var context =new MLContext(seed: 1);\n",
        "var pipeline = context.Transforms.Concatenate(\"Features\", \"X\")\n",
        "    .Append(context.Auto().Regression(\"y\", useLbfgs: false, useSdca: false, useFastForest: false));\n",
        "\n",
        "var monitor = new NotebookMonitor();\n",
        "var experiment = context.Auto().CreateExperiment();\n",
        "experiment.SetPipeline(pipeline)\n",
        "        .SetEvaluateMetric(RegressionMetric.RootMeanSquaredError, \"y\")\n",
        "        .SetTrainingTimeInSeconds(300)\n",
        "        .SetDataset(trainTestSplit.TrainSet, trainTestSplit.TestSet)\n",
        "        .SetMonitor(monitor);\n",
        "\n",
        "// Configure Visualizer\t\t\t\n",
        "monitor.SetUpdate(monitor.Display());\n",
        "\n",
        "var res = await experiment.RunAsync();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check model/metric from experiment result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "dotnet-interactive.csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"dni-plaintext\">38236.134331302615</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// get model\n",
        "var model  = res.Model;\n",
        "var eval = model.Transform(trainTestSplit.TestSet);\n",
        "var metric = context.Regression.Evaluate(eval, \"y\");\n",
        "\n",
        "// should be identical with res.Metric\n",
        "metric.RootMeanSquaredError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Continue learning\n",
        "\n",
        "> [⏩ Next Module - Model Evaluation](https://raw.githubusercontent.com/JakeRadMSFT/csharp-notebooks/main/machine-learning/04-Model%20Evaluation.ipynb)  \n",
        "> [⏪ Last Module - Data Prep and Feature Engineering](https://raw.githubusercontent.com/JakeRadMSFT/csharp-notebooks/main/machine-learning/02-Data%20Preparation%20and%20Feature%20Engineering.ipynb)  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".NET (C#)",
      "language": "C#",
      "name": ".net-csharp"
    },
    "language_info": {
      "file_extension": ".cs",
      "mimetype": "text/x-csharp",
      "name": "C#",
      "pygments_lexer": "csharp",
      "version": "8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
